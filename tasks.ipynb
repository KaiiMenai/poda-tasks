{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles of Data Analytics - Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Source the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sourcing the data for use during the module.\n",
    "\n",
    "The Iris dataset can be found here: https://gist.github.com/curran/a08a1080b88344b0c8a7\n",
    "\n",
    "A way to make it easier for the csv to be read easier is found here: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frames.\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning Library that contains example datasets.\n",
    "import sklearn as skl\n",
    "\n",
    "# Import other libraries that will help with visualising and analysing data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data frames and machine learning libraries have been imported. It's now time to import the dataset using read.csv. This will be task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will load in the iris dataset from the web link.\n",
    "\n",
    "df = pd.read_csv(\"https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Explore the Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to check that the dataset is actually loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the dataset.\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, need to check the shape of the data, this will help further along when deciding the types of exploratory statistical tests to conduct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 150 samples, each of these samples has had a number of measurements taken (5 variables in all) - sepal width and length, petal width and length, and species of iris. Based on Stevens (1946) categarisation of data types, these are forms of ratio (length and width measurements), and nominal (species). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ease analysis it may be plausible to standardise the dataset, particularly the measurements. This may aid in analysis further down the line. Could do a tukeys analysis later to see if there is significant diference in petal/sepal length and/or width, between the species depending on iris species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and last 5 rows of the dataset are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_df = pd.concat([df.head(5), df.tail(5)])\n",
    "print(sliced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting some information on what is included in the dataset. Although I have previously stated that the data is ratio and nominal. This will clarify the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information table, it can be seen that one column has categorical (nominal) data (species column) and all the other columns are of the numeric type with non-Null entriesm that is, there are no 0 / null~ entries in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking for null values and the different data types, now checking for missing values is important. If there were missing values, the dataset would need to be cleaned and sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following a checking of the dataset, it was found that there were no missing values in any of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the data is correct and contains no repitition of recorded values it was important to check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop_duplicates(subset =\"species\",)\n",
    "\n",
    "print(data.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No duplicates were found in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it was important to check if the dataset was balanced to ensure that the analysis isn't skewed by the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts(\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that all the species contain equal amounts of rows, thus the dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Summarize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next - quick statistical summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also need to check the class distribution to make sure that nothing will be skewed during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClass distribution:\")\n",
    "print(df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the species and the number of samples recorded for each. It can be seen that there were 50 samples recorded for each of the three species; setosa, versicolor, and virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Visualize Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a number of different plotting methods the data can be visualised. Due to the nature of the measurements taken for the data, it may be suitable to split the dataset for some plots into those looking at species vs. sepal or petal width/length, and using a boxplot to measure the length vs. width of sepal or petal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization\n",
    "# Pairplot colored by species\n",
    "sns.pairplot(df, hue='species', height=2.5)\n",
    "plt.suptitle(\"Pairwise Feature Relationships\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations on plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next need to boxplot and seperate by species.\n",
    "\n",
    "and add error bars to the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots by species\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, feature in enumerate(['sepal_length', 'sepal_width', 'petal_length', 'petal_width']):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.boxplot(x='species', y=feature, data=df)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Investigate Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to conduct relationship investigations is through Principal Component Analysis (PCA) - I did this for my PhD research and found it was a great way to clearly loot at multiple data aspects at one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Principal Component Analysis\n",
    "X = df.iloc[:, :4]\n",
    "y = df['species']\n",
    "# However, we need to standardise the data before performing PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note of PCA is that the data needs to be standardised for it. \n",
    "When standardising data, it's important that it is scaled correctly, otherwise the results will appear skewed and purely incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis can then be run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then view the standardised PCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=principal_components[:, 0], y=principal_components[:, 1], \n",
    "                hue=df['species'], palette='viridis', s=100)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "plt.title(\"PCA of Iris Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Analyze Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do the relationships show? \n",
    "\n",
    "#### Data Structure:\n",
    "\n",
    "150 samples with 4 features (all numerical)\n",
    "\n",
    "3 balanced classes (50 samples per species)\n",
    "\n",
    "No missing values\n",
    "\n",
    "\n",
    "#### PCA Insights:\n",
    "\n",
    "First 2 components explain 95.8% of variance\n",
    "\n",
    "PC1 (73% variance) strongly correlates with petal measurements\n",
    "\n",
    "PC2 (22.8% variance) relates to sepal width\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Analyze Class Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specifically analyse the data seen within each species, the data needs to be seperated out into the 3 respective species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Compute Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess whether there are similarities or differences between the species and the length/width of the sepals and petals, it is important to conduct analysis into the relationship between sepal length and width, and petal length and with against the particular species. This will also help further down the line when designing a method to predict the species of an iris flower from measurements alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation Analysis\n",
    "corr_matrix = df.iloc[:, :4].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot, it can be seen that the warmer the colour/higher the number, closer to 1 in the range (-0.4 - 1), the greater the correlation with the other feature analysed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Relationships:\n",
    "\n",
    "Petal measurements show strong positive correlation (r=0.96)\n",
    "\n",
    "Sepal width has lowest correlation with other features\n",
    "\n",
    "Setosa is distinctly different in petal measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Fit a Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for sepal linear regression\n",
    "X_sepal = df[['sepal_length']]\n",
    "y_sepal = df['sepal_width']\n",
    "\n",
    "# Split the dataset\n",
    "X_sepal_train, X_sepal_test, y_sepal_train, y_sepal_test = train_test_split(X_sepal, y_sepal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the model\n",
    "model_sepal = LinearRegression()\n",
    "model_sepal.fit(X_sepal_train, y_sepal_train)\n",
    "\n",
    "# Make predictions\n",
    "y_sepal_pred = model_sepal.predict(X_sepal_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_sepal = mean_squared_error(y_sepal_test, y_sepal_pred)\n",
    "r2_sepal = r2_score(y_sepal_test, y_sepal_pred)\n",
    "\n",
    "print(\"Sepal Linear Regression Results:\")\n",
    "print(f\"Coefficient: {model_sepal.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {model_sepal.intercept_:.4f}\")\n",
    "print(f\"Mean squared error: {mse_sepal:.4f}\")\n",
    "print(f\"R-squared score: {r2_sepal:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_sepal_test, y_sepal_test, color='blue', alpha=0.5, label='Actual data')\n",
    "plt.plot(X_sepal_test, y_sepal_pred, color='red', label='Regression line')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.title('Linear Regression: Sepal Length vs Sepal Width')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was then used to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction for sepal model\n",
    "new_sepal_length = np.array([[5.7]])  # New sepal length value\n",
    "predicted_sepal_width = model_sepal.predict(new_sepal_length)\n",
    "print(\"\\nSepal Width Prediction:\")\n",
    "print(f\"For a sepal length of 5.7 cm, the predicted sepal width is: {predicted_sepal_width[0]:.2f} cm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the same was done with the petal length and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for petal linear regression\n",
    "X_petal = df[['petal_length']]\n",
    "y_petal = df['petal_width']\n",
    "\n",
    "# Split the dataset\n",
    "X_petal_train, X_petal_test, y_petal_train, y_petal_test = train_test_split(X_petal, y_petal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the model\n",
    "model_petal = LinearRegression()\n",
    "model_petal.fit(X_petal_train, y_petal_train)\n",
    "\n",
    "# Make predictions\n",
    "y_petal_pred = model_petal.predict(X_petal_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_petal = mean_squared_error(y_petal_test, y_petal_pred)\n",
    "r2_petal = r2_score(y_petal_test, y_petal_pred)\n",
    "\n",
    "print(\"\\nPetal Linear Regression Results:\")\n",
    "print(f\"Coefficient: {model_petal.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {model_petal.intercept_:.4f}\")\n",
    "print(f\"Mean squared error: {mse_petal:.4f}\")\n",
    "print(f\"R-squared score: {r2_petal:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_petal_test, y_petal_test, color='green', alpha=0.5, label='Actual data')\n",
    "plt.plot(X_petal_test, y_petal_pred, color='red', label='Regression line')\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "plt.title('Linear Regression: Petal Length vs Petal Width')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prediction was also made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction for petal model\n",
    "new_petal_length = np.array([[4.5]])  # New petal length value\n",
    "predicted_petal_width = model_petal.predict(new_petal_length)\n",
    "print(\"\\nPetal Width Prediction:\")\n",
    "print(f\"For a petal length of 4.5 cm, the predicted petal width is: {predicted_petal_width[0]:.2f} cm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Too Many Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a linear regression model has previously been added to the data, it is possible to predict a species based on the measurements taken through using a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic model for species classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for species classification\n",
    "X_species = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y_species = df['species']\n",
    "\n",
    "# Encode species names to numerical values\n",
    "le = LabelEncoder()\n",
    "y_species = le.fit_transform(y_species)\n",
    "\n",
    "# Split the dataset\n",
    "X_species_train, X_species_test, y_species_train, y_species_test = train_test_split(X_species, y_species, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the model\n",
    "model_species = LogisticRegression(max_iter=200)\n",
    "model_species.fit(X_species_train, y_species_train)\n",
    "\n",
    "# Make predictions\n",
    "y_species_pred = model_species.predict(X_species_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_species_test, y_species_pred)\n",
    "\n",
    "print(\"\\nLogistic Regression for Species Classification Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_species_test, y_species_pred, target_names=le.classes_))\n",
    "\n",
    "# Example prediction\n",
    "example_data = [[5.1, 3.5, 1.4, 0.2]]  # sepal_length, sepal_width, petal_length, petal_width\n",
    "predicted_species = model_species.predict(example_data)\n",
    "print(f\"\\nPredicted species for {example_data[0]}: {le.inverse_transform(predicted_species)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance:\n",
    "\n",
    "Logistic regression achieves ~97% accuracy\n",
    "\n",
    "Virginica shows slightly lower recall due to overlap with Versicolor\n",
    "\n",
    "Most confusion occurs between Versicolor and Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why choose this model?\n",
    "Reason:\n",
    "\n",
    "Simple baseline for multi-class classification\n",
    "\n",
    "Interpretable coefficients\n",
    "\n",
    "Works well with small, linearly separable datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "\n",
    "Precision/Recall: Measures class-specific performance\n",
    "\n",
    "F1-score: Balanced metric for imbalanced classes (though Iris is balanced)\n",
    "\n",
    "Confusion Matrix: Visualizes error patterns between similar classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap='Blues', \n",
    "            xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stevens, S. S. 1946. On the Theory of Scales of Measurement. Science, Vol. 103, No. 2648. http://expsylab.psych.uoa.gr/fileadmin/expsylab.psych.uoa.gr/uploads/papers/Stevens_1946.pdf\n",
    "\n",
    "Python. 2025. Built-in types. https://docs.python.org/3/library/stdtypes.html\n",
    "\n",
    "Stackoverflow user \"Life is complex\". First and last n rows of a dataframe. https://stackoverflow.com/questions/58260771/how-to-show-firstlast-n-rows-of-a-dataframe\n",
    "\n",
    "Thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELP DOCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/iris-dataset/\n",
    "\n",
    "https://www.geeksforgeeks.org/exploratory-data-analysis-on-iris-dataset/\n",
    "\n",
    "https://stackoverflow.com/questions/58260771/how-to-show-firstlast-n-rows-of-a-dataframe\n",
    "\n",
    "https://bytemedirk.medium.com/mastering-iris-dataset-analysis-with-python-9e040a088ef4\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html\n",
    "\n",
    "https://medium.com/@kachiann/a-beginners-guide-to-machine-learning-with-python-iris-flower-prediction-61814e095268\n",
    "\n",
    "https://www.geeksforgeeks.org/python-basics-of-pandas-using-iris-dataset/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
